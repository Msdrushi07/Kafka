Perfect 🙌 Let’s build a ready-to-use Spring Boot Kafka project structure that covers:

✅ Producer (publishes messages)

✅ Consumer (processes messages)

✅ Error Handling (retry + DLQ)

✅ DLQ Consumer (logs failed messages)

✅ DLQ Reprocessor (re-sends messages back to main topic)



---

📂 Project Structure (Maven)

spring-kafka-demo/
 ├── src/main/java/com/example/kafka/
 │    ├── config/
 │    │     └── KafkaErrorHandlerConfig.java
 │    ├── consumer/
 │    │     ├── OrderConsumer.java
 │    │     └── DlqConsumer.java
 │    ├── producer/
 │    │     └── OrderProducer.java
 │    ├── reprocessor/
 │    │     └── DlqReprocessor.java
 │    └── SpringKafkaDemoApplication.java
 └── src/main/resources/
      └── application.yml


---

⚙️ application.yml

spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: order-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer
        spring.json.trusted.packages: "*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer


---

🛠️ KafkaErrorHandlerConfig.java

package com.example.kafka.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.util.backoff.FixedBackOff;

@Configuration
public class KafkaErrorHandlerConfig {

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<Object, Object> template) {
        DeadLetterPublishingRecoverer recoverer =
                new DeadLetterPublishingRecoverer(template); // sends to <topic>.DLT

        // Retry 3 times, 2s apart, then send to DLQ
        FixedBackOff fixedBackOff = new FixedBackOff(2000L, 3);

        return new DefaultErrorHandler(recoverer, fixedBackOff);
    }
}


---


import org.apache.kafka.clients.admin.NewTopic;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class KafkaTopicConfig {

    @Bean
    public NewTopic ordersTopic() {
        return new NewTopic("orders", 3, (short) 1); 
        // 3 partitions, replication factor 1
    }

    @Bean
    public NewTopic ordersDltTopic() {
        return new NewTopic("orders.DLT", 3, (short) 1);
    }
}


---
📨 OrderProducer.java

package com.example.kafka.producer;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class OrderProducer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    public void sendOrder(String order) {
        kafkaTemplate.send("orders", order);
        System.out.println("✅ Sent order: " + order);
    }
}


---

📥 OrderConsumer.java

package com.example.kafka.consumer;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

@Service
public class OrderConsumer {

    @KafkaListener(topics = "orders", groupId = "order-group")
    public void consume(String message, Acknowledgment ack) {
        try {
            System.out.println("Processing order: " + message);

            if (message.contains("fail")) {
                throw new RuntimeException("Simulated failure: " + message);
            }

            ack.acknowledge(); // commit offset only if success
        } catch (Exception e) {
            // ❌ don’t ack → triggers errorHandler (retries + DLQ)
            throw e;
        }
    }
}


---

❌ DlqConsumer.java (Monitor DLQ)

package com.example.kafka.consumer;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class DlqConsumer {

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-group")
    public void consumeDlq(String message) {
        System.out.println("🚨 Message moved to DLQ: " + message);
    }
}


---

♻️ DlqReprocessor.java (Re-send from DLQ → Main)

package com.example.kafka.reprocessor;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class DlqReprocessor {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-reprocessor")
    public void reprocess(String message) {
        System.out.println("♻️ Reprocessing DLQ message: " + message);

        try {
            if (message.contains("bad")) {
                System.out.println("⚠️ Skipping permanently bad message: " + message);
                return;
            }

            kafkaTemplate.send("orders", message);
            System.out.println("✅ Resent message to orders: " + message);

        } catch (Exception e) {
            System.err.println("❌ Reprocessing failed: " + e.getMessage());
        }
    }
}


---

🚀 SpringKafkaDemoApplication.java

package com.example.kafka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SpringKafkaDemoApplication {
    public static void main(String[] args) {
        SpringApplication.run(SpringKafkaDemoApplication.class, args);
    }
}


---

🔄 How It Works

1. Normal flow →
Producer → orders topic → Consumer processes → Ack on success.


2. Failure flow →
Consumer fails → retried 3 times → message goes to orders.DLT.


3. DLQ monitoring →
DlqConsumer logs failed messages.


4. Reprocessing →
DlqReprocessor listens to DLQ → re-sends valid messages to orders.




---

using command creating topics
# Create main topic
kafka-topics.sh --create --topic orders --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092

# Create DLQ topic
kafka-topics.sh --create --topic orders.DLT --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092

Perfect 👌 Rushi — here’s a starter Grafana dashboard JSON you can directly import.
This dashboard focuses on Kafka consumer lag, throughput, and DLQ monitoring for your Spring Boot + Micrometer + Prometheus setup.


---

📊 Grafana Dashboard JSON (Kafka Consumer Monitoring)

{
  "id": null,
  "title": "Kafka Consumer Monitoring",
  "tags": ["kafka", "spring-boot", "prometheus"],
  "timezone": "browser",
  "schemaVersion": 16,
  "version": 1,
  "panels": [
    {
      "type": "graph",
      "title": "Consumer Lag per Topic",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "kafka_consumer_records_lag",
          "legendFormat": "{{topic}} - {{client_id}}",
          "refId": "A"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "Messages Consumed per Second",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "rate(kafka_consumer_records_consumed_total[1m])",
          "legendFormat": "{{topic}}",
          "refId": "B"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "Messages Published per Second",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "rate(kafka_producer_record_send_total[1m])",
          "legendFormat": "{{topic}}",
          "refId": "C"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "DLQ Messages Rate",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "increase(kafka_consumer_records_consumed_total{topic=\"orders.DLT\"}[5m])",
          "legendFormat": "orders.DLT",
          "refId": "D"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "singlestat",
      "title": "Consumer App Up",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "up{job=\"spring-kafka-app\"}",
          "refId": "E"
        }
      ],
      "colorBackground": true,
      "colors": ["#d44a3a", "rgba(237, 129, 40, 0.89)", "#299c46"],
      "valueName": "max",
      "thresholds": "0,1"
    }
  ]
}


---

🔹 What This Dashboard Shows

1. Consumer Lag per Topic → shows if consumers are falling behind.


2. Messages Consumed/sec → throughput of your consumers.


3. Messages Published/sec → throughput of producers.


4. DLQ Messages Rate → spikes mean errors are happening.


5. Consumer App Up → checks if your Spring Boot Kafka app is alive.




---

🚀 Steps to Import

1. Go to Grafana → Dashboards → + → Import.


2. Paste the JSON above or upload as a file.


3. Select your Prometheus datasource.


4. Done 🎉 — you’ll see live Kafka monitoring!







