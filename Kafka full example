Perfect ğŸ™Œ Letâ€™s build a ready-to-use Spring Boot Kafka project structure that covers:

âœ… Producer (publishes messages)

âœ… Consumer (processes messages)

âœ… Error Handling (retry + DLQ)

âœ… DLQ Consumer (logs failed messages)

âœ… DLQ Reprocessor (re-sends messages back to main topic)



---

ğŸ“‚ Project Structure (Maven)

spring-kafka-demo/
 â”œâ”€â”€ src/main/java/com/example/kafka/
 â”‚    â”œâ”€â”€ config/
 â”‚    â”‚     â””â”€â”€ KafkaErrorHandlerConfig.java
 â”‚    â”œâ”€â”€ consumer/
 â”‚    â”‚     â”œâ”€â”€ OrderConsumer.java
 â”‚    â”‚     â””â”€â”€ DlqConsumer.java
 â”‚    â”œâ”€â”€ producer/
 â”‚    â”‚     â””â”€â”€ OrderProducer.java
 â”‚    â”œâ”€â”€ reprocessor/
 â”‚    â”‚     â””â”€â”€ DlqReprocessor.java
 â”‚    â””â”€â”€ SpringKafkaDemoApplication.java
 â””â”€â”€ src/main/resources/
      â””â”€â”€ application.yml


---

âš™ï¸ application.yml

spring:
  kafka:
    bootstrap-servers: localhost:9092
    consumer:
      group-id: order-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer
        spring.json.trusted.packages: "*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer


---

ğŸ› ï¸ KafkaErrorHandlerConfig.java

package com.example.kafka.config;

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.util.backoff.FixedBackOff;

@Configuration
public class KafkaErrorHandlerConfig {

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<Object, Object> template) {
        DeadLetterPublishingRecoverer recoverer =
                new DeadLetterPublishingRecoverer(template); // sends to <topic>.DLT

        // Retry 3 times, 2s apart, then send to DLQ
        FixedBackOff fixedBackOff = new FixedBackOff(2000L, 3);

        return new DefaultErrorHandler(recoverer, fixedBackOff);
    }
}


---


import org.apache.kafka.clients.admin.NewTopic;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class KafkaTopicConfig {

    @Bean
    public NewTopic ordersTopic() {
        return new NewTopic("orders", 3, (short) 1); 
        // 3 partitions, replication factor 1
    }

    @Bean
    public NewTopic ordersDltTopic() {
        return new NewTopic("orders.DLT", 3, (short) 1);
    }
}


---
ğŸ“¨ OrderProducer.java

package com.example.kafka.producer;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class OrderProducer {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    public void sendOrder(String order) {
        kafkaTemplate.send("orders", order);
        System.out.println("âœ… Sent order: " + order);
    }
}


---

ğŸ“¥ OrderConsumer.java

package com.example.kafka.consumer;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

@Service
public class OrderConsumer {

    @KafkaListener(topics = "orders", groupId = "order-group")
    public void consume(String message, Acknowledgment ack) {
        try {
            System.out.println("Processing order: " + message);

            if (message.contains("fail")) {
                throw new RuntimeException("Simulated failure: " + message);
            }

            ack.acknowledge(); // commit offset only if success
        } catch (Exception e) {
            // âŒ donâ€™t ack â†’ triggers errorHandler (retries + DLQ)
            throw e;
        }
    }
}


---

âŒ DlqConsumer.java (Monitor DLQ)

package com.example.kafka.consumer;

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class DlqConsumer {

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-group")
    public void consumeDlq(String message) {
        System.out.println("ğŸš¨ Message moved to DLQ: " + message);
    }
}


---

â™»ï¸ DlqReprocessor.java (Re-send from DLQ â†’ Main)

package com.example.kafka.reprocessor;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class DlqReprocessor {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-reprocessor")
    public void reprocess(String message) {
        System.out.println("â™»ï¸ Reprocessing DLQ message: " + message);

        try {
            if (message.contains("bad")) {
                System.out.println("âš ï¸ Skipping permanently bad message: " + message);
                return;
            }

            kafkaTemplate.send("orders", message);
            System.out.println("âœ… Resent message to orders: " + message);

        } catch (Exception e) {
            System.err.println("âŒ Reprocessing failed: " + e.getMessage());
        }
    }
}


---

ğŸš€ SpringKafkaDemoApplication.java

package com.example.kafka;

import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;

@SpringBootApplication
public class SpringKafkaDemoApplication {
    public static void main(String[] args) {
        SpringApplication.run(SpringKafkaDemoApplication.class, args);
    }
}


---

ğŸ”„ How It Works

1. Normal flow â†’
Producer â†’ orders topic â†’ Consumer processes â†’ Ack on success.


2. Failure flow â†’
Consumer fails â†’ retried 3 times â†’ message goes to orders.DLT.


3. DLQ monitoring â†’
DlqConsumer logs failed messages.


4. Reprocessing â†’
DlqReprocessor listens to DLQ â†’ re-sends valid messages to orders.




---

using command creating topics
# Create main topic
kafka-topics.sh --create --topic orders --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092

# Create DLQ topic
kafka-topics.sh --create --topic orders.DLT --partitions 3 --replication-factor 1 --bootstrap-server localhost:9092

Perfect ğŸ‘Œ Rushi â€” hereâ€™s a starter Grafana dashboard JSON you can directly import.
This dashboard focuses on Kafka consumer lag, throughput, and DLQ monitoring for your Spring Boot + Micrometer + Prometheus setup.


---

ğŸ“Š Grafana Dashboard JSON (Kafka Consumer Monitoring)

{
  "id": null,
  "title": "Kafka Consumer Monitoring",
  "tags": ["kafka", "spring-boot", "prometheus"],
  "timezone": "browser",
  "schemaVersion": 16,
  "version": 1,
  "panels": [
    {
      "type": "graph",
      "title": "Consumer Lag per Topic",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "kafka_consumer_records_lag",
          "legendFormat": "{{topic}} - {{client_id}}",
          "refId": "A"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "Messages Consumed per Second",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "rate(kafka_consumer_records_consumed_total[1m])",
          "legendFormat": "{{topic}}",
          "refId": "B"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "Messages Published per Second",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "rate(kafka_producer_record_send_total[1m])",
          "legendFormat": "{{topic}}",
          "refId": "C"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "graph",
      "title": "DLQ Messages Rate",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "increase(kafka_consumer_records_consumed_total{topic=\"orders.DLT\"}[5m])",
          "legendFormat": "orders.DLT",
          "refId": "D"
        }
      ],
      "lines": true,
      "linewidth": 2
    },
    {
      "type": "singlestat",
      "title": "Consumer App Up",
      "datasource": "Prometheus",
      "targets": [
        {
          "expr": "up{job=\"spring-kafka-app\"}",
          "refId": "E"
        }
      ],
      "colorBackground": true,
      "colors": ["#d44a3a", "rgba(237, 129, 40, 0.89)", "#299c46"],
      "valueName": "max",
      "thresholds": "0,1"
    }
  ]
}


---

ğŸ”¹ What This Dashboard Shows

1. Consumer Lag per Topic â†’ shows if consumers are falling behind.


2. Messages Consumed/sec â†’ throughput of your consumers.


3. Messages Published/sec â†’ throughput of producers.


4. DLQ Messages Rate â†’ spikes mean errors are happening.


5. Consumer App Up â†’ checks if your Spring Boot Kafka app is alive.




---

ğŸš€ Steps to Import

1. Go to Grafana â†’ Dashboards â†’ + â†’ Import.


2. Paste the JSON above or upload as a file.


3. Select your Prometheus datasource.


4. Done ğŸ‰ â€” youâ€™ll see live Kafka monitoring!







