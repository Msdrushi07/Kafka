Perfect ðŸ‘Œ letâ€™s set up a Spring Boot consumer service that listens to two different Kafka clusters at the same time.


---

ðŸ”¹ 1. application.yml

Here we define two clusters under different prefixes:

spring:
  kafka:
    cluster1:
      bootstrap-servers: localhost:9092
      consumer:
        group-id: group-cluster1
        auto-offset-reset: earliest
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

    cluster2:
      bootstrap-servers: localhost:9093
      consumer:
        group-id: group-cluster2
        auto-offset-reset: earliest
        key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
        value-deserializer: org.apache.kafka.common.serialization.StringDeserializer

âš¡ Notice: we donâ€™t use the default spring.kafka directly. Instead, we separate configs for cluster1 and cluster2.


---

ðŸ”¹ 2. Java Config (Multiple Factories)

import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactory;
import org.springframework.kafka.core.ConsumerFactory;
import org.springframework.kafka.core.DefaultKafkaConsumerFactory;

import java.util.HashMap;
import java.util.Map;

@Configuration
public class MultiKafkaConsumerConfig {

    // ----- Cluster 1 -----
    @Value("${spring.kafka.cluster1.bootstrap-servers}")
    private String cluster1Bootstrap;

    @Value("${spring.kafka.cluster1.consumer.group-id}")
    private String cluster1GroupId;

    @Bean
    public ConsumerFactory<String, String> cluster1ConsumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster1Bootstrap);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, cluster1GroupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> cluster1KafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(cluster1ConsumerFactory());
        return factory;
    }

    // ----- Cluster 2 -----
    @Value("${spring.kafka.cluster2.bootstrap-servers}")
    private String cluster2Bootstrap;

    @Value("${spring.kafka.cluster2.consumer.group-id}")
    private String cluster2GroupId;

    @Bean
    public ConsumerFactory<String, String> cluster2ConsumerFactory() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, cluster2Bootstrap);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, cluster2GroupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        return new DefaultKafkaConsumerFactory<>(props);
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> cluster2KafkaListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(cluster2ConsumerFactory());
        return factory;
    }
}


---

ðŸ”¹ 3. Kafka Listeners

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Service;

@Service
public class MultiClusterConsumerService {

    @KafkaListener(
        topics = "orders", 
        containerFactory = "cluster1KafkaListenerContainerFactory"
    )
    public void consumeFromCluster1(String message) {
        System.out.println("Cluster 1 - orders: " + message);
    }

    @KafkaListener(
        topics = "payments", 
        containerFactory = "cluster2KafkaListenerContainerFactory"
    )
    public void consumeFromCluster2(String message) {
        System.out.println("Cluster 2 - payments: " + message);
    }
}


---

ðŸ”¹ How this works

Cluster 1:

Producers publish to topic orders on Kafka running at localhost:9092.

Consumer picks it up via cluster1KafkaListenerContainerFactory.


Cluster 2:

Producers publish to topic payments on Kafka running at localhost:9093.

Consumer picks it up via cluster2KafkaListenerContainerFactory.



ðŸ‘‰ One microservice, two Kafka clusters, completely separate.


---

âœ… Summary

Yes, a single Spring Boot microservice can consume from multiple Kafka clusters.

The trick: define multiple consumer factories + listener container factories, one per cluster.

Then assign each listener to the correct factory.



---

Do you also want me to extend this example so that one listener consumes from multiple topics inside the same cluster, while another listens to a different cluster? Thatâ€™s a very common real-world pattern.


Excellent question ðŸ‘Œ â€” this is exactly the real integration challenge.
You are right:

Upstream Kafka might be Cluster A.

DROID internal Kafka might be Cluster B (Confluent Kafka backbone).

TIBCO EMS is a completely different technology (JMS).


So how does DROID consume from both?
ðŸ‘‰ By using multiple consumer configurations inside Spring Boot.


---

ðŸ”¹ Case 1: Consuming from Multiple Kafka Clusters

Spring Kafka lets you define multiple ConsumerFactory + KafkaListenerContainerFactory beans.

Example:

@Configuration
public class KafkaConfig {

    @Bean
    public Map<String, Object> kafkaClusterAProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "clusterA:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "droid-clusterA-consumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        return props;
    }

    @Bean
    public ConsumerFactory<String, String> kafkaClusterAConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(kafkaClusterAProps());
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaClusterAListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(kafkaClusterAConsumerFactory());
        return factory;
    }

    // --- Cluster B (internal DROID Kafka) ---
    @Bean
    public Map<String, Object> kafkaClusterBProps() {
        Map<String, Object> props = new HashMap<>();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "clusterB:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "droid-clusterB-consumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        return props;
    }

    @Bean
    public ConsumerFactory<String, String> kafkaClusterBConsumerFactory() {
        return new DefaultKafkaConsumerFactory<>(kafkaClusterBProps());
    }

    @Bean
    public ConcurrentKafkaListenerContainerFactory<String, String> kafkaClusterBListenerContainerFactory() {
        ConcurrentKafkaListenerContainerFactory<String, String> factory = new ConcurrentKafkaListenerContainerFactory<>();
        factory.setConsumerFactory(kafkaClusterBConsumerFactory());
        return factory;
    }
}

Then in your consumers:

@Service
public class MultiClusterConsumer {

    @KafkaListener(
        topics = "trade-events",
        containerFactory = "kafkaClusterAListenerContainerFactory"
    )
    public void consumeFromClusterA(String message) {
        System.out.println("Received from Kafka Cluster A: " + message);
        // normalize & forward to internal DROID Kafka (Cluster B)
    }

    @KafkaListener(
        topics = "droid-normalized-trades",
        containerFactory = "kafkaClusterBListenerContainerFactory"
    )
    public void consumeFromClusterB(String message) {
        System.out.println("Received from internal DROID Kafka Cluster B: " + message);
    }
}

ðŸ‘‰ This way, DROID can consume from external Kafka (Cluster A) and republish to internal Kafka (Cluster B).


---

ðŸ”¹ Case 2: Consuming from TIBCO EMS (JMS)

TIBCO EMS is JMS-based. In Spring Boot you configure a JmsListenerContainerFactory:

@Configuration
public class EmsConfig {

    @Bean
    public ConnectionFactory emsConnectionFactory() {
        // TIBCO EMS connection setup
        return new com.tibco.tibjms.TibjmsConnectionFactory("tcp://ems-server:7222");
    }

    @Bean
    public JmsListenerContainerFactory<?> emsListenerContainerFactory(ConnectionFactory emsConnectionFactory) {
        DefaultJmsListenerContainerFactory factory = new DefaultJmsListenerContainerFactory();
        factory.setConnectionFactory(emsConnectionFactory);
        return factory;
    }
}

Consumer:

@Service
public class EmsConsumer {

    @JmsListener(destination = "EMS.TRADE.QUEUE", containerFactory = "emsListenerContainerFactory")
    public void consumeFromEms(String message) {
        System.out.println("Received from EMS: " + message);
        // normalize & forward to DROID Kafka (Cluster B)
    }
}


---

ðŸ”¹ How it all fits in DROID

1. External Kafka cluster (Cluster A) â†’ DROID consumer reads â†’ republishes normalized event to internal Kafka (Cluster B).


2. EMS queue â†’ DROID JMS consumer reads â†’ republishes normalized event to internal Kafka (Cluster B).


3. Internal DROID Kafka (Cluster B) = single backbone for all downstream systems (Oracle, ES, S3, APIs).




---

âœ… So you are right â€” DROID must know connection details for each cluster/system and configure a separate listener container factory.

âš¡ Key takeaway:

DROID acts as a bridge between heterogeneous upstreams and a unified downstream Kafka backbone.



---




