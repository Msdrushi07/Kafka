Event-driven Order System with Kafka (Spring Boot)

A realistic, production-lean example showing who publishes and who consumes across four services:

Order Service (HTTP API) — accepts orders, orchestrates via events (choreography-lite)

Inventory Service — reserves/releases stock

Payment Service — authorizes/captures payment

Notification Service — emails/SMS/push


We use topics and event contracts to keep services decoupled. Each service has its own DB and communicates only through Kafka events.


---

1) Topics & Ownership

Topic	Key	Produced by	Consumed by	Purpose

orders.v1	orderId	Order Service	Inventory, Payment, Notification	New orders created/updated (e.g., OrderCreated, OrderCancelled)
inventory.v1	orderId	Inventory Service	Order, Notification	Stock reservation results (InventoryReserved, InventoryReservationFailed)
payments.v1	orderId	Payment Service	Order, Notification	Payment results (PaymentAuthorized, PaymentFailed)
fulfillment.v1	orderId	Order Service	Notification	Final state (OrderCompleted, OrderFailed)


> Rule: The service that owns the business action produces the event on its domain topic.




---

2) Event Contracts (JSON examples)

2.1 orders.v1

{
  "type": "OrderCreated",
  "orderId": "ord_123",
  "customerId": "cust_9",
  "items": [{"sku": "SKU-1", "qty": 2}],
  "total": 1499.00,
  "currency": "INR",
  "createdAt": "2025-08-15T10:30:00Z",
  "traceId": "b5f..."
}

2.2 inventory.v1

{
  "type": "InventoryReserved",
  "orderId": "ord_123",
  "reservations": [{"sku": "SKU-1", "qty": 2}],
  "reservedAt": "2025-08-15T10:30:02Z",
  "traceId": "b5f..."
}

{
  "type": "InventoryReservationFailed",
  "orderId": "ord_123",
  "reason": "OUT_OF_STOCK",
  "traceId": "b5f..."
}

2.3 payments.v1

{
  "type": "PaymentAuthorized",
  "orderId": "ord_123",
  "amount": 1499.00,
  "currency": "INR",
  "paymentId": "pay_777",
  "traceId": "b5f..."
}

{
  "type": "PaymentFailed",
  "orderId": "ord_123",
  "reason": "CARD_DECLINED",
  "traceId": "b5f..."
}

2.4 fulfillment.v1

{
  "type": "OrderCompleted",
  "orderId": "ord_123",
  "traceId": "b5f..."
}

{
  "type": "OrderFailed",
  "orderId": "ord_123",
  "reason": "PAYMENT_FAILED|OUT_OF_STOCK",
  "traceId": "b5f..."
}

> In prod, prefer Avro/Protobuf + Schema Registry. Add headers: type, traceId, correlationId.




---

3) End‑to‑End Flow (Happy Path)

1. Order Service (HTTP POST /orders) persists Order with state PENDING and publishes OrderCreated → orders.v1.


2. Inventory Service consumes OrderCreated, reserves stock, persists reservation, and publishes InventoryReserved → inventory.v1.


3. Payment Service consumes OrderCreated or (safer, sequential) InventoryReserved. It authorizes payment and publishes PaymentAuthorized → payments.v1.


4. Order Service consumes InventoryReserved and PaymentAuthorized (it tracks both flags in its DB). When both done → sets state COMPLETED and publishes OrderCompleted → fulfillment.v1.


5. Notification Service consumes from all domain topics to send emails/SMS at appropriate moments.



Failure/Compensation Paths

If InventoryReservationFailed → Order sets FAILED, publishes OrderFailed, (optional) Payment Service ignores/voids.

If PaymentFailed after reservation → Order sets FAILED, publishes OrderFailed, Inventory releases reservation (reacts to OrderFailed).


> This pattern is a Saga (choreography): each service reacts to events and performs the next step, including compensations.




---

4) Spring Boot — Minimal Code per Service

Below are condensed snippets to show producers/consumers. Replace JSON with your serializer (Jackson/Avro).

4.1 Common Gradle/Maven deps

<!-- Order/Inventory/Payment/Notification each include: -->
<dependency>
  <groupId>org.springframework.kafka</groupId>
  <artifactId>spring-kafka</artifactId>
</dependency>
<dependency>
  <groupId>org.springframework.boot</groupId>
  <artifactId>spring-boot-starter-validation</artifactId>
</dependency>

4.2 application.yml (example)

spring:
  kafka:
    bootstrap-servers: localhost:9092
    properties:
      enable.idempotence: true
      acks: all
    consumer:
      group-id: inventory-svc
      auto-offset-reset: earliest
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    producer:
      retries: 10
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      key-serializer: org.apache.kafka.common.serialization.StringSerializer

spring.kafka.producer.retries: 5
spring.kafka.producer.properties.retry.backoff.ms: 1000

4.3 Order Service

Publishes OrderCreated; Consumes InventoryReserved, PaymentAuthorized; Publishes OrderCompleted/Failed.

// OrderController.java
@PostMapping("/orders")
public ResponseEntity<OrderDto> create(@Valid @RequestBody CreateOrderRequest req){
    Order order = orderService.createPendingOrder(req); // DB: PENDING
    events.publish("orders.v1", order.getId(), new OrderCreated(order));
    return ResponseEntity.accepted().body(OrderDto.from(order));
}

// OrderEvents.java (producer)
@Service
@RequiredArgsConstructor
public class OrderEvents {
  private final KafkaTemplate<String, String> kafka;
  private final ObjectMapper om;
  public void publish(String topic, String key, Object event){
    try { kafka.send(topic, key, om.writeValueAsString(event)); }
    catch (JsonProcessingException e) { throw new RuntimeException(e); }
  }
}

// OrderSagaListener.java (consumer)
@Component
@RequiredArgsConstructor
public class OrderSagaListener {
  private final OrderRepository repo;
  private final OrderEvents events;
  private final ObjectMapper om;

  @KafkaListener(topics = {"inventory.v1"}, groupId = "order-svc")
  public void onInventory(String payload, @Header("type") String type) throws Exception {
    if ("InventoryReserved".equals(type)) {
      InventoryReserved ev = om.readValue(payload, InventoryReserved.class);
      repo.markInventoryOk(ev.getOrderId());
      tryComplete(ev.getOrderId());
    } else if ("InventoryReservationFailed".equals(type)) {
      InventoryReservationFailed ev = om.readValue(payload, InventoryReservationFailed.class);
      repo.markFailed(ev.getOrderId(), "OUT_OF_STOCK");
      events.publish("fulfillment.v1", ev.getOrderId(), new OrderFailed(ev.getOrderId(), "OUT_OF_STOCK"));
    }
  }

  @KafkaListener(topics = {"payments.v1"}, groupId = "order-svc")
  public void onPayment(String payload, @Header("type") String type) throws Exception {
    if ("PaymentAuthorized".equals(type)) {
      PaymentAuthorized ev = om.readValue(payload, PaymentAuthorized.class);
      repo.markPaymentOk(ev.getOrderId());
      tryComplete(ev.getOrderId());
    } else if ("PaymentFailed".equals(type)) {
      PaymentFailed ev = om.readValue(payload, PaymentFailed.class);
      repo.markFailed(ev.getOrderId(), "PAYMENT_FAILED");
      events.publish("fulfillment.v1", ev.getOrderId(), new OrderFailed(ev.getOrderId(), "PAYMENT_FAILED"));
    }
  }

  private void tryComplete(String orderId){
    repo.findById(orderId).filter(o -> o.isInventoryOk() && o.isPaymentOk()).ifPresent(o -> {
      o.setStatus(OrderStatus.COMPLETED);
      repo.save(o);
      events.publish("fulfillment.v1", orderId, new OrderCompleted(orderId));
    });
  }
}

4.4 Inventory Service

Consumes OrderCreated; Publishes reservation result.

@Component
@RequiredArgsConstructor
public class InventoryListener {
  private final InventoryDomain domain; // checks stock, reserves
  private final KafkaTemplate<String, String> kafka;
  private final ObjectMapper om;

  @KafkaListener(topics = "orders.v1", groupId = "inventory-svc")
  public void onOrderCreated(String payload, @Header("type") String type) throws Exception {
    if (!"OrderCreated".equals(type)) return;
    OrderCreated ev = om.readValue(payload, OrderCreated.class);
    boolean reserved = domain.tryReserve(ev.getOrderId(), ev.getItems());
    Object out = reserved ? new InventoryReserved(ev.getOrderId(), ev.getItems())
                          : new InventoryReservationFailed(ev.getOrderId(), "OUT_OF_STOCK");
    kafka.send("inventory.v1", ev.getOrderId(), om.writeValueAsString(out));
  }
}

4.5 Payment Service

Consumes InventoryReserved (or OrderCreated if you want parallelization); Publishes payment result.

@Component
@RequiredArgsConstructor
public class PaymentListener {
  private final PaymentGateway gateway;
  private final KafkaTemplate<String, String> kafka;
  private final ObjectMapper om;

  @KafkaListener(topics = "inventory.v1", groupId = "payment-svc")
  public void onInventory(String payload, @Header("type") String type) throws Exception {
    if (!"InventoryReserved".equals(type)) return;
    InventoryReserved ev = om.readValue(payload, InventoryReserved.class);
    PaymentResult pr = gateway.authorize(ev.getOrderId());
    Object out = pr.isOk() ? new PaymentAuthorized(ev.getOrderId(), pr.getAmount(), pr.getPaymentId())
                           : new PaymentFailed(ev.getOrderId(), pr.getReason());
    kafka.send("payments.v1", ev.getOrderId(), om.writeValueAsString(out));
  }
}

4.6 Notification Service

Consumes from all topics and sends email/SMS.

@Component
@RequiredArgsConstructor
public class NotificationListener {
  private final Mailer mailer;

  @KafkaListener(topics = {"orders.v1","inventory.v1","payments.v1","fulfillment.v1"}, groupId = "notification-svc")
  public void onAny(String payload, @Header("type") String type) {
    // Inspect type and send appropriate notification
  }
}


---

5) Reliability Patterns (Prod‑ready)

Idempotency: Use orderId as key; make handlers idempotent (upserts with unique constraints). Store a processed_event_id table to ignore duplicates.

Outbox Pattern: Write event to an outbox table in the same DB transaction as state change; a background publisher relays to Kafka. Prevents lost events.

Consumer Groups & Partitions: Key by orderId so all events for an order go to the same partition; scale consumers horizontally.

Exactly‑once semantics: Enable producer idempotence + transactions if you need atomic write-to-DB-and-produce (or leverage outbox).

Retries & DLQ: Configure max.poll.interval.ms, seekToCurrentErrorHandler (Spring Kafka) or new DefaultErrorHandler with DLQ topics like payments.dlq.v1.

Observability: Add traceId to headers; export metrics (consumer lag, success/fail counts) to Prometheus + Grafana; centralize logs.



---

6) Who Publishes vs Who Consumes (Cheat Sheet)

Order Service: Publishes OrderCreated, OrderCompleted/Failed; Consumes InventoryReserved/Failed, PaymentAuthorized/Failed.

Inventory Service: Consumes OrderCreated; Publishes InventoryReserved/Failed; Consumes OrderFailed (to release holds).

Payment Service: Consumes InventoryReserved (or OrderCreated); Publishes PaymentAuthorized/Failed; Consumes OrderFailed (to void/cancel).

Notification Service: Consumes everything; Publishes nothing.



---

7) Local Dev Tips

Start Kafka quickly with docker-compose (Redpanda/Bitnami images work well).

Use a schema registry for evolvable contracts.

Include contract tests for event payloads.

Seed realistic data and run a happy-path + compensation-path test.



---

8) docker-compose (dev example)

version: "3.9"
services:
  kafka:
    image: bitnami/kafka:3.7
    ports: ["9092:9092"]
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_PROCESS_ROLES=broker,controller
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@localhost:9093


---

Wrap‑up

This setup gives you a clean, testable event-driven flow: each service owns its domain logic, publishes results, and reacts to others — no tight coupling, and failures trigger compensations automatically. Copy the snippets into separate Spring Boot services and you’ll have a working skeleton you can extend.


Producer Side Issues

1. Broker Unavailability (message not published)

Problem: Producer cannot reach Kafka brokers.

Solution:

Configure retries with exponential backoff:

spring.kafka.producer.retries: 5
spring.kafka.producer.properties.retry.backoff.ms: 1000

Use acks=all (wait for replication).

Use Kafka bootstrap servers list with multiple brokers, not just one.




---

2. Serialization Errors

Problem: Object → JSON/Avro serialization fails.

Solution:

Use proper serializers in Spring Boot:

spring.kafka.producer.value-serializer: org.springframework.kafka.support.serializer.JsonSerializer

Add schema validation (Avro + Schema Registry).




---

3. Message Loss (acks misconfigured)

Problem: Messages acknowledged too early.

Solution:

Set acks=all for strong durability.

Enable idempotence to avoid duplicates:

spring.kafka.producer.properties.enable.idempotence: true




---

4. Message Duplication

Problem: Retries without idempotence cause duplicates.

Solution:

Use idempotent producers (enable.idempotence=true).

Ensure deduplication logic in consumers if required.




---

5. Throughput Bottleneck

Problem: Producer too slow.

Solution:

Tune batching:

spring.kafka.producer.properties.linger.ms: 5
spring.kafka.producer.properties.batch.size: 32768

Use compression (compression.type: snappy or lz4).




---

🔹 Consumer Side Issues

6. Consumer Not Consuming

Problem: Wrong group ID, topic mismatch, subscription issue.

Solution:

Use unique group.id.

Check topic exists via kafka-topics.sh --describe.




---

7. Offset Mismanagement

Problem: Messages reprocessed or skipped.

Solution:

Disable auto-commit and use manual acknowledgment:

@KafkaListener(topics = "orders")
public void listen(String msg, Acknowledgment ack) {
    try {
        process(msg);
        ack.acknowledge(); // commit offset only if success
    } catch (Exception e) {
        // retry or send to DLQ
    }
}




---

8. Consumer Lag

Problem: Consumer slower than producer.

Solution:

Scale consumers (increase partitions + consumer count).

Tune max.poll.records and fetch.min.bytes.

Monitor lag using Kafka Exporter + Prometheus + Grafana.




---

9. Deserialization Errors

Problem: Consumer cannot parse message.

Solution:

Use Spring Kafka’s ErrorHandlingDeserializer:

spring.kafka.consumer.value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
spring.kafka.consumer.properties.spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer

Add schema evolution rules if using Avro.




---

10. Poison Pill Messages (crash loop)

Problem: A bad message keeps crashing consumer.

Solution:

Configure a Dead Letter Topic (DLT) in Spring Kafka:

spring.kafka.listener.default-error-handler-dlq: true

Or use a SeekToCurrentErrorHandler in code:

@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<Object, Object> template) {
    return new DefaultErrorHandler(new DeadLetterPublishingRecoverer(template), new FixedBackOff(1000L, 3));
}

This retries 3 times, then sends bad messages to DLT.




---

11. Rebalancing Issues

Problem: Frequent rebalances → downtime.

Solution:

Use static group membership:

spring.kafka.consumer.properties.group.instance.id: consumer-1

Tune session timeouts:

spring.kafka.consumer.properties.session.timeout.ms: 30000
spring.kafka.consumer.properties.max.poll.interval.ms: 300000




---

🔹 System/Infra Issues

12. Leader Not Available / Network Issues

Solution: Retries + multiple brokers in bootstrap.



---

13. Disk Full (retention issues)

Solution:

Monitor disk usage.

Use retention policies:

log.retention.hours=168   # 7 days
log.segment.bytes=1073741824




---

14. Data Skew (uneven partition load)

Solution:

Choose partition key wisely (e.g., userId, orderId).

Avoid always sending to partition-0.

Use custom partitioner if needed.




---

⚙️ Retry Mechanisms in Spring Boot + Kafka

Producer Retry (built-in)

Kafka producer retries are automatic with:

spring.kafka.producer.retries: 5
spring.kafka.producer.properties.retry.backoff.ms: 1000

Consumer Retry + DLQ

Spring Kafka provides DefaultErrorHandler:

@Bean
public DefaultErrorHandler errorHandler(KafkaTemplate<Object, Object> template) {
    // retry 2 times, then send to DLQ
    return new DefaultErrorHandler(
        new DeadLetterPublishingRecoverer(template), 
        new FixedBackOff(2000L, 2)
    );
}


---

✅ So, if I summarize:

Producer side: retries + idempotence + correct ack.

Consumer side: manual ack + error handler + DLQ.

Infra side: partition strategy + monitoring (Prometheus + Grafana).


in real-world Kafka projects because “bad” messages (poison pills) can crash or block your consumers if not handled properly.


---

🔹 Error Handling in Spring Kafka

In Spring Kafka, you typically deal with errors at the listener (consumer) side. By default:

If deserialization fails → consumer crashes.

If your @KafkaListener throws an exception → consumer retries indefinitely.


👉 That’s why we configure a DefaultErrorHandler (earlier called SeekToCurrentErrorHandler).


---

✅ Step 1: Configure Error Handling

Create a Spring bean for error handling:

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.listener.DefaultErrorHandler;
import org.springframework.kafka.listener.DeadLetterPublishingRecoverer;
import org.springframework.util.backoff.FixedBackOff;

@Configuration
public class KafkaErrorHandlerConfig {

    @Bean
    public DefaultErrorHandler errorHandler(KafkaTemplate<Object, Object> template) {
        // DeadLetterPublishingRecoverer will send failed messages to <original-topic>.DLT
        DeadLetterPublishingRecoverer recoverer = new DeadLetterPublishingRecoverer(template);

        // Retry 3 times, with 2 seconds delay, then send to DLQ
        FixedBackOff fixedBackOff = new FixedBackOff(2000L, 3);

        return new DefaultErrorHandler(recoverer, fixedBackOff);
    }
}

🔍 What happens here?

When a message fails to process:

1. Spring will retry 3 times (every 2s).


2. If still failing → message goes to a DLQ (Dead Letter Topic).


3. The DLQ topic will have the same name as the original topic, with .DLT suffix (e.g., orders → orders.DLT).





---

✅ Step 2: Configure Producer & Consumer Serializers

DLQ relies on sending failed messages back to Kafka, so we need proper serializers:

spring:
  kafka:
    consumer:
      group-id: order-group
      auto-offset-reset: earliest
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.springframework.kafka.support.serializer.ErrorHandlingDeserializer
      properties:
        spring.deserializer.value.delegate.class: org.springframework.kafka.support.serializer.JsonDeserializer
        spring.json.trusted.packages: "*"
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.springframework.kafka.support.serializer.JsonSerializer


---

✅ Step 3: Use KafkaListener with Manual Ack (Optional)

If you want more control (commit offsets only on success):

import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.support.Acknowledgment;
import org.springframework.stereotype.Service;

@Service
public class OrderConsumer {

    @KafkaListener(topics = "orders", groupId = "order-group")
    public void consume(String message, Acknowledgment ack) {
        try {
            System.out.println("Processing order: " + message);

            if (message.contains("fail")) {
                throw new RuntimeException("Simulated failure for: " + message);
            }

            // Only acknowledge if processed successfully
            ack.acknowledge();

        } catch (Exception e) {
            // Don't ack → triggers error handler (retry + DLQ)
            throw e;
        }
    }
}


---

✅ Step 4: Consuming from DLQ

The DLQ topic (orders.DLT) will automatically be created. You can set up another listener to monitor DLQ:

@Service
public class DlqConsumer {

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-group")
    public void consumeDlq(String message) {
        System.out.println("❌ Message moved to DLQ: " + message);
        // You can log, alert, or persist this message for manual investigation
    }
}


---

🔹 Summary

Retries: Use DefaultErrorHandler with FixedBackOff (or ExponentialBackOff).

DLQ: Use DeadLetterPublishingRecoverer → failed messages go to <topic>.DLT.

Deserialization Errors: Use ErrorHandlingDeserializer.

Recovery: Consume from DLQ to investigate or reprocess

By default, the Dead Letter Queue (DLQ) is a final resting place for failed messages.
But in real projects, you may want to:

1. Investigate why a message failed (manual review).


2. Fix the root cause (like a bug, schema mismatch, or bad data).


3. Reprocess/retry messages from DLQ back to the main topic.




---

🔹 Reprocessing Messages from DLQ

There are 2 common strategies:


---

✅ Strategy 1: Manual Reprocessing (Simple + Controlled)

Consume messages from DLQ, transform/fix them, and publish back to the original topic.

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Service;

@Service
public class DlqReprocessor {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    @KafkaListener(topics = "orders.DLT", groupId = "dlq-reprocessor")
    public void reprocess(String message) {
        System.out.println("♻️ Reprocessing DLQ message: " + message);

        try {
            // 🔧 Optional: apply fix/validation before re-publishing
            if (message.contains("bad")) {
                System.out.println("Skipping bad message: " + message);
                return;
            }

            // Publish back to the original topic
            kafkaTemplate.send("orders", message);

        } catch (Exception e) {
            System.err.println("Reprocessing failed: " + e.getMessage());
            // You can push to another topic (like orders.PERMANENT.DLT)
        }
    }
}

👉 Here:

Messages in orders.DLT get picked up.

If valid, re-sent to orders.

If still bad, you may log it or move to another permanent DLQ.



---

✅ Strategy 2: Scheduled / On-Demand Reprocessing

Sometimes you don’t want DLQ to auto-retry immediately (might cause loops).
Instead, reprocess messages on-demand via a REST API.

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.web.bind.annotation.*;

@RestController
@RequestMapping("/dlq")
public class DlqController {

    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;

    @PostMapping("/reprocess")
    public String reprocess(@RequestParam String topic, @RequestParam String message) {
        kafkaTemplate.send(topic, message);
        return "Reprocessed message back to " + topic;
    }
}

👉 This way:

You fetch messages from DLQ (e.g., via Kafka UI tool or consumer).

Call this API → message goes back to original topic.



---

✅ Strategy 3: Dedicated Retry Topics (More Advanced)

Some teams prefer separate retry topics instead of DLQ, e.g.:

orders → main topic

orders.retry → retry after delay

orders.DLT → permanent dead letter queue


You can configure Spring Kafka with a RetryTopicConfiguration (built-in support):

import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.kafka.config.ConcurrentKafkaListenerContainerFactoryConfigurer;
import org.springframework.kafka.retrytopic.RetryTopicConfiguration;
import org.springframework.kafka.retrytopic.RetryTopicConfigurationBuilder;

@Configuration
public class RetryConfig {

    @Bean
    public RetryTopicConfiguration retryTopicConfig(ConcurrentKafkaListenerContainerFactoryConfigurer configurer) {
        return RetryTopicConfigurationBuilder
                .newInstance()
                .maxAttempts(4) // retries + main attempt
                .fixedBackOff(2000) // 2 sec delay
                .create("orders")   // applies to "orders" topic
                .build();
    }
}

👉 Spring will automatically create:

orders-retry-0, orders-retry-1, … (for retries).

orders.DLT (for final failures).



---

🔹 Summary

Default DLQ (orders.DLT): store failed messages.

Manual Reprocessing (Strategy 1): auto-consume DLQ → publish back.

On-Demand Reprocessing (Strategy 2): via REST API → republish when needed.

Retry Topics (Strategy 3): automatic retry with backoff, then DLQ.

